.. _commands:

Cloud Deploy Commands
=====================

.. toctree::
    :maxdepth: 2

Build | buildimage
------------------

| This command should be the first one triggered after an Application creation.
| It bakes a new AMI with every feature specified in the application on top of the source AMI chosen (generally a Claranet Debian AMI).
| Claranet uses SaltStack and Ansible to provision all the chosen features and uses Packer (from HashiCorp) to bake the new AMI.
| Note: The Ansible provisioner use the ansible-galaxy command to get roles. The base ansible repository need to be the one containing the requirements.yml file.
|

**Command Options**

*Instance Type*:
  ``string``

 Choose the instance type of the temporary instance created by Packer for provisionning.

*Skip Provisioner bootstrap* :
  ``boolean``

 This option permits to choose if Packer should install the Salt agent (minion) or/and ansible before applying all feature formulas.

**Life cycle hooks**

    There are two Hooks available and configurable in the application.
    Those hooks are scripts:

        * ``pre_buildimage``: Script executed on the temporary instance *before* the features provisioning
        * ``post_buildimage``: Script executed on the temporary instance *after* the features provisioning

.. figure:: /images/ghost_buildimage.png

.. figure:: /images/buildimage_workflow.png


Build | updatelifecyclehooks
----------------------------

This command allows you to upload and upgrade the pre-bootstrap and post-bootstrap scripts defined in the application.
Those scripts will be executed when a new instance is created by the Auto Scaling Group or when the ``createinstance`` command is triggered.

Here is a workflow diagram for more details:

.. figure:: /images/bootstrap_workflow.png

Build | createinstance
----------------------

This command allows you to create a standalone instance (not attached to an Auto Scaling Group).
You can validate and test your module deployments and the AMI generated by ``buildimage``

**Command Options**

*Subnet*:
  ``subnet_id``

 This option permits to choose in which subnet the instance must be created.

*Private IP address*:
  ``empty by default``

 If specified, this option permits to choose the Private IP address to affect to the created instance.


Build | destroyallinstances
---------------------------

This command allows you to destroy all the instances mapped to the current application (based on EC2 Tags: ``app``, ``env``, ``role``, ``color``).
Both *standalone* and *in AutoScale* instances are destroyed.

.. _cmd_recreateinstances:

Build | recreateinstances
-------------------------

This command allows you to renew all the instances associated to an Application.
It is useful when you have made changes in the baking or bootstrapping steps of your application instances and want to apply them with automation.
The command works for any application in any context, with or without Auto Scaling Group. Here are the cases:

Application without an Auto Scaling Group
*****************************************

		* Destroy all standalone instances
		* Wait until the instances release their private IPs
		* Create new instances using subnet and private IP matching the old destroyed ones

Application with an Auto Scaling Group and one or many LoadBalancers
********************************************************************

*Without Rolling update strategy option*:

		* Suspend the ASG's processes
		* Deregister the instances from the Load Balancer

		.. warning:: Without rolling update strategy this may produce a downtime

		* Double the target and max value of the ASG
		* Resume the ``Launch`` process, let the ASG creates new instances
		* Wait until all the new instances are up, Healthy and InService in the Load Balancer
		* Detach the oldest instances from the Load Balancer, wait for the connection draining
		* Destroy the oldest instances, set back ASG original target and max values, and resume ``Terminate`` process
		* Wait until all the old instances are destroyed and detached from the ASG.
		* Resume all ASG processes

*With a Rolling update strategy option*:

		* Split the instances into groups (see next chapter for details), and apply the previous workflow on each group one after the other.

**Command Options**

*Rolling update strategy*:
  ``1by1 | 50% | 1/3 | 25%``

 This option permits to use the selected strategy during the rolling update operation

 *This option can't be used if there is only one instance to recreate*

*The rolling update options are:*

		* 1by1: Instances will be processed one after the other.
		* 50%:  The whole instances will be split in 2 groups (the more equals) .
		* 1/3:  The whole instances will be split in 3 groups (the more equals).
		* 25%:  The whole instances will be split in 4 groups (the more equals).

Run/Build | updateautoscaling
-----------------------------

This command allows you to create a new ``LaunchConfiguration`` with every parameter taken from the application and attach it to the Auto Scaling Group.
It will also update the Auto Scaling Group parameters (update ``min``, ``max``) and tags (set needed application's tags).


.. _cmd_deploy:

Run | deploy
------------

This command allows you to deploy a new configuration version or a new revision of the application.
You can choose the revision you want to deploy (git branch, tag, or hash commit).

Here is a workflow diagram for more details:

.. figure:: /images/deploy_workflow.png


**Command Options**

*Strategy*:
  ``serial | parallel``

 This option permits to choose if the SSH connection to all instances should occur in parallel or not.

*Safe Deploy*:
  ``boolean and parameters``

.. _cmd_deploy_safe:

Safe deploy documentation:
**************************

 The Safe Deployment feature aims to create a sweet way to deploy on EC2 instances. Currently when you perform a deployment,
 the instances stay in the Load Balancer's pool during the operation even if the deployment process reload or restart a service. It's not a really efficient and safe
 because it can break user sessions and create a bad user experience.

 It's to tackle this problem that the safe deployment feature as been created. The vision is, for an Auto scaling Group, split the instances in multiple groups.
 The feature gives the possibility to choose the split strategy of the whole Auto Scaling instances and so create multiple instance groups.
 Each instance groups will be removed from their Load Balancer before being deployed in a standard way and then registered.

 The safe deployment feature can be used with the commands "deploy" and "redeploy" and can be used with the fabric strategy.


 For Auto Scaling Group with one or more HAproxy as Load Balancer, the safe deployment process works with Hapi only.

*The safe deployment possibilities are:*

		* 1by1: Instances will be processed one after the other.
		* 50%:  The whole instances will be split in 2 groups(the more equals) .
		* 1/3:  The whole instances will be split in 3 groups(the more equals).
		* 25%:  The whole instances will be split in 4 groups(the more equals).

 The safe deployment must be configured in the application.

 Application Parameters (In the new Load Balancer section) you can choose:

		* The type of the Load Balancer (CLB, ALB or Haproxy) used by this application.
		  (To identify the Load Balancers, if the type is an ELB or an ALB, it's the Auto Scaling Group parameters which will identify them).

 For an HAproxy type, other information will be required (app tag, the HAproxy backend).
		* The "Time to wait before deployment"(seconds) which is the time to wait after the instances deregistration process, to allow the client to finalize their requests.
		  (If the Load Balancer is an ELB or an ALB, this value will be added to the connection draining value of the ELB)
		* The "Time to wait after deployment"(seconds) which is the time to wait after having been deployed. After this time the instances will be registered
		  in their Load Balancer.

		Only if the Load Balancer type is HAproxy:

		* The "HAproxy app tag" is the tag value set for the HAproxy instance which you want to associate to this application.
		  (The search will be performed against instances with the tags: app (this value), env (auto retrieves; same as the application), role (fixed: 'loadbalancer'))
		* The "HAProxy backend name" is the name of the backend where the instances are defined in the HAproxy configuration.
		* The "HAproxy API port" is the HAproxy API listening port.

 To activate it for a deployment, it's on the commands side. You can used it when you "deploy" or "redeploy" a module.

 Commands Parameters:
		* There is check box, "Deploy with Safe Deployment", to activate it.
		* The parameter "Safe Deployment Strategy" will appear when the check box is checked. It shows the safe deployment possibilities (dynamically identify by the number of currently running instances).

 The process safe deployment process is:

		* Check that all instances in the Load Balancer (ELB or HAproxy) are in service and are enough to perform the safe deployment and check also, when there are multiple Load Balancers, that they have the same configuration.
		* Split the instances list in groups according the deployment type chosen (1by1-1/3-25%-50%).
		* Before beginning to deploy on the instances group, remove them from their Load Balancer (Haproxy or ELB).
		* Wait a moment (depends on the connection draining value for the ELB and/or the custom value defined).
		* Launch the standard deployment process.
		* Wait a moment (depends on the custom value defined).
		* Add the updated instances in their Load Balancer.
		* Wait until instances become healthy (checked every 10s).
		* Do the same process for the next instance groups.

Example of safe deployment output:

		* Suspending Auto Scaling Group processes ['Terminate', 'Launch']
		* Instances [u'i-87bc5b05', u'i-87bcei97'] well deregistered in the ELB test
		* Waiting 15s: The connection draining time more the custom value set for wait_before_deploy
		* Updating current instances in serial: [u'10.10.12.80', u'10.10.11.50']
		* Waiting 10s: The value set for wait_after_deploy
		* Instances [u'i-87bc5b05', u'i-87bcei97'] well registered in the ELB test
		* Waiting 10s because the instance is not in service in the ELB
		* Waiting 10s because the instance is not in service in the ELB
		* Instances: [u'10.10.10.189', u'10.10.11.50'] have been deployed and are registered in their ELB
		* ...
		* ... (same process for the others safe deployment groups)
		* ...
		* Resuming Auto Scaling Group processes ['Terminate', 'Launch']

 In case of failure during the safe deployment process, if the instances were disabled/deregistered from their Load Balancer, they stay in this state. At the next deploy/redeploy action, a new complete deployment will be performed on them and if no error occurs, they will be enabled/registered in their Load Balancer.


Here is a workflow diagram for more details:

.. figure:: /images/safe_deploy_workflow_1.png

Step 1)
 Split instances into Groups (25%, 33%, 50% or 1 instance in the group) and handle each group

.. figure:: /images/safe_deploy_workflow_2.png

Step 2)
 Remove the current group's instances from the Load Balancer - no more traffic on those instances

.. figure:: /images/safe_deploy_workflow_3.png

Step 3)
 Standard module deploy on every detached instance of the group (serial or parallel deploy)

.. figure:: /images/safe_deploy_workflow_4.png

Step 4)
 Put back instances online in the Load Balancer, wait for HealthCheck becoming green/OK

.. figure:: /images/safe_deploy_workflow_5.png

Step 5-1)
 Go on next group, and repeat the workflow


Run | redeploy
--------------

This command allows you to deploy an old version that has already been deployed. This command is useful in order to rollback a bad deployment of an application or a system configuration.

When you choose to use this command, you have to specify which deployment you want to re-deploy.
This archive will be brought back (generated by the ``buildpack`` step from the ``deploy`` command) and executed in the half second part of the ``deploy`` workflow:

		* Executes pre deploy script
		* Symlinks the deployment content
		* Executes post deploy script
		* Notification (by mail)

.. seealso:: Please see :ref:`Run | deploy <cmd_deploy>` documentation for more details.

.. _cmd_executescript:

Run | executescript
-------------------

This command allows you to run a custom shell script on running instance(s) in the application.

.. warning:: All modification made by this command are volatile and will not be applied to instances created after the script execution. Which means you have to use the classical deploy workflow in order to persist any kind of modification on your instances.

**Command Options**

*Script*:
  ``b64 content``

 The script file (base64 encoded at API level) to run

*Module context*:
  ``string`` - can be empty or null

 The module name to use if set. The script file will be executed in the module target path/directory, useful if you need to run or interact with existing module's scripts.
 It will also execute the script with the UID/user configured in the module (or with **root** by default if no user UID is set).

*Execution Strategy*:
  ``string: single | serial | parallel``

 This option permits to choose if the SSH connection would be to a unique instance, or to all instances in parallel/serial.

*Strategy param: Single Host IP or Safe Deploy Group option*:
  ``string: private IPv4`` - mandatory when using **single** in Execution Strategy

 This option permits to specify a single instance and launch the command only on this instance. When this parameter is set (private IPv4 address), other parameters will be ignored (Safe option).

  ``Parameters for Safe deployment strategy`` - can be null or empty (uses a unique group with every instance), or a grouping strategy (Please refer to **Safe deployment documentation**)

 This option permits to use the Safe Deploy workflow to execute the specific script on each pool of instances.


.. seealso:: Please see :ref:`Run | deploy <cmd_deploy>` documentation for more details about :ref:`Safe deployment strategies and flow <cmd_deploy_safe>`


Blue/Green | preparebluegreen
-----------------------------

This command is the first step in the ``Blue/Green`` deployment process.
It will duplicate the current **online** Elastic Load Balancer to create a temporary one and attach this temporary ELB to the **stand by** Auto Scaling Group.
It will also update the **stand by** Auto Scaling Group (Please see **Run/Build | updateautoscaling** for more details.)
After this command, you can update the **stand by** environment by triggering ``buildimage`` and ``deploy`` commands.

**Command Options**

*Copy AMI*:
  ``boolean``

 This option permits to duplicate the AMI id from the **online** Auto Scaling Group when updating the **stand by** Auto Scaling Group.

Blue/Green | swapbluegreen
--------------------------

This command is the main step in the ``Blue/Green`` deployment process.
It will swap the two versions of the application by swapping the ``blue`` and ``green`` environments.

**Command Options**

*Swap strategy*:
  ``overlap | isolated``

    This is the Blue/Green swap strategy to choose:

    * ``overlap``: Default behavior. There is no downtime of service with this strategy but both versions of the application could be online at the same time because the **N+1** version will be attached to the **online** ELB before the **N** version is detached.

    * ``isolated``: This strategy will produce a service downtime but ensures that only one version of the application is online.


Blue/Green | purgebluegreen
---------------------------

This command is the last step in the ``Blue/Green`` deployment process.
It will purge the **stand by** environment by deleting the temporary ELB, updating the **stand by** Auto Scaling Group and destroying all instances for this environment.

.. seealso:: Please see **Build | destroyallinstances** for more details
